{
  "name": "簡單使用rvest進行網路爬取",
  "tagline": "一個從沒用R寫過爬蟲的R信徒，快速入門學習筆記",
  "body": "> # 前言  \r\n\r\n\r\n由於工作上的的需要，小弟近期自學使用R語言進行爬蟲，在眾多大神的神文中沈浮，所以想自行編輯一篇文章來讓各位有興趣也如同我沒有使用R爬蟲過的菜鳥能少踩點雷，有一條快速的路徑能迅速上手。然而，在使用過程中，其實小弟也遇到一些難題，本文在後面將會做一小段的小分享，關於使用rvest爬蟲的缺點。\r\n\r\n進行爬蟲的過程中，需要一些網頁編輯架構的基礎知識，也就是所謂的 **HTML**與 **CSS** 等編輯網頁的語言，而在本篇小弟取自其他大大的神文中小弟認為簡單明暸的說明文，希望能快速的讓大家通過這一個階段！\t\r\n\r\n\r\n## 本文適合對象\r\n\r\n* 曾經使用過R，並且熟悉`dplyr`與`Pipeline`者為佳\r\n\r\n## 自身背景簡介\r\n\r\n* 身份：役男(剛從貓空統計所畢業) \r\n* R年齡：約兩年左右\r\n* 是否具備 **html** 知識：NO \r\n* 是否具備 **css**  知識：NO\r\n* 曾經使用過的R套件：dplyr、tidyr、ggplot2、...etc\r\n\r\n\r\n## 挑選 **rvest** 作為爬蟲套件的原因\r\n\r\n* **rvest** 的作者與我最常使用的一些套件為同一開發者，所以有許多特性相似也相容。\r\n* **hadley**的威名就不用小編多談惹ＸＤ\r\n* **rvest** 支援了 **Pipeline** 的設計 (這要用過的同道們才懂呀QQ)\r\n\r\n> # 正片開始\r\n\r\n## 網頁基本架構介紹與SelectorGadget的應用\r\n\r\n* HTML就如同一間空房子\r\n* CSS就像是房子的裝潢部分，材質越好，花的時間越久，當然就會越漂亮。\r\n* jQuery這邊就先不提\r\n\r\n下面連結為在使用 **rvest** 前所需內容：\r\n\r\n### [CSS與HTML基本介紹與SelecttorGadget](https://blog.gtwang.org/r/rvest-web-scraping-with-r/)\r\n\r\n以上連結為某位大大的神文，基本上當您看完上面連結的前兩頁，就可以準備進入爬蟲的階段，而在接下來的部分，我將給予各種範例並寫上註解來幫助各位了解爬蟲過程與Combo的過程。\r\n\r\n### 套件的函數說明\r\n\r\n在進行**rvest**套件的使用前，先以維基百科的某個頁面當作材料，介紹一些常用的function，經過這個說明可以減少各位在R中使用或閱讀`?XXXX()`的次數XD。\r\n\r\n```\r\n## read_html函數先將整個網頁的原始HTML程式碼抓下來\r\npage.source <- read_html(\"https://en.wikipedia.org/wiki/R_(programming_language)\")\r\n## 執行這個物件，看看他的內容，其實可以視為這個網址下的網頁內容背後的原始html\r\npage.source\r\n```\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-04%20%E4%B8%8A%E5%8D%8811.02.17.png)\r\n\r\n```\r\n## 配合google得CSS選擇器將指定的資料取出來\r\nversion.block <- html_nodes(page.source, \".wikitable th+ td , th:nth-child(2) , .wikitable th:nth-child(1)\")\r\n## 亦可使用xpath的方式抓取\r\nversion.block2 <- html_nodes(page.source, xpath = '//table[@class=\"wikitable\"]//tr//th')\r\nhead(version.block)\r\n```\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-04%20%E4%B8%8B%E5%8D%889.55.01.png)\r\n\r\n\r\n```\r\n## 萃取中元素中的相關資訊\r\ncontent <- html_text(version.block)\r\nhead(content)\r\n```\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-04%20%E4%B8%8B%E5%8D%8810.19.36.png)\r\n\r\n```\r\n## 萃取出所有html的元素標籤\r\nhtml_name(version.block)\r\n```\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-04%20%E4%B8%8B%E5%8D%8810.26.51.png)\r\n\r\n```\r\n## 可以列出每個 HTML 元素的所有屬性\r\nel.attrs <- html_attrs(version.block)\r\n## 也可以列出特定屬性\r\nel.attr <- html_attr(version.block, \"style\")\r\nhead(el.attr)\r\n```\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-04%20%E4%B8%8B%E5%8D%8810.36.10.png)\r\n\r\n---\r\n\r\n\r\n## 範例二\r\n### 爬取[職棒球員的統計表](http://www.cpbl.com.tw/stats/all.html?&year=2016&stat=pbat&sort=AVG&order=desc)中，球員數據表\r\n\r\n#### 目的：爬取表中各球員的**OBP**、**SLG**、**AVG**\r\n\r\n#### 步驟如下註解：\r\n\r\n```{r}\r\n## 要先讀入這個url的html，存入cpbl_url這個物件中\r\ncpbl_url = read_html(\"http://www.cpbl.com.tw/stats/all.html?&year=2016&stat=pbat&sort=AVG&order=desc\")\r\n\r\n## 將整個爬取資料存入新的物件\r\nnba_url_name <- nba_url %>% \r\n  html_nodes(css = \"td a\") %>% ##萃取出CSS選擇器的資料\r\n  html_text() %>% ## 取出資料\r\n  data.frame(NAME = .) ## 將資料存成dataframe，並命名欄位名稱為NAME\r\n\r\n## 以下與上面意義相同\r\nnba_url %>%\r\n  html_nodes(css = \"td:nth-child(16)\") %>%\r\n  html_text() %>%\r\n  data.frame(OBP = .) -> nba_url_OBP\r\n\r\nnba_url %>%\r\n  html_nodes(css = \"td:nth-child(17)\") %>%\r\n  html_text() %>%\r\n  data.frame(SLG = .) -> nba_url_SLG\r\n\r\nnba_url %>%\r\n  html_nodes(css = \"td:nth-child(18)\") %>%\r\n  html_text() %>%\r\n  data.frame(AVG = .) -> nba_url_AVG\r\n  \r\n ## 最後將整理完的資料合併\r\n\r\nnba_url_name %>% \r\n  bind_cols(nba_url_OBP) %>% \r\n  bind_cols(nba_url_SLG) %>% \r\n  bind_cols(nba_url_AVG) %>% \r\n  head() ## 看看前6筆資料就好\r\n  \r\n```\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-03%20%E4%B8%8B%E5%8D%888.20.54.png)\r\n\r\n## 範例三\r\n\r\n### [爬取露天的商品資料](http://www.ruten.com.tw/)\r\n\r\n範例三所示範的內容步驟如下：\r\n\r\n1. 自動在露天官網輸入欲查詢的商品關鍵字\r\n2. 得到該商品的網頁連結\r\n3. 爬取該連結內的商品資料\r\n\r\n步驟如下註解：\r\n\r\n```\r\n## 模擬一個網頁瀏覽的session\r\nse <- html_session(\"http://www.ruten.com.tw/\")\r\nse\r\n```\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-05%20%E4%B8%8A%E5%8D%8810.55.33.png)\r\n\r\n```\r\n## 解析網頁表格部分，但注意這只是將表格部分拿出來填\r\nfo <- html_form(se)[[1]]\r\nfo\r\n```\r\n\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-05%20%E4%B8%8A%E5%8D%8811.02.10.png)\r\n\r\n可以發現`<input text> 'k'`的部分其實就是官網首頁中，輸入關鍵字的部分。\r\n\r\n```\r\n## 在fo中為'k'的這個位置輸入'耳機'(i.e.意思就是在關鍵字搜尋格輸入耳機)\r\nfo <- set_values(fo, k = '耳機') \r\nfo\r\n```\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-05%20%E4%B8%8A%E5%8D%8811.08.00.png)\r\n\r\n可以發現在`'k'`的部分被填入耳機並存入`fo`這個物件，下一步就是提交這個form\r\n\r\n```\r\n## 這個function是在rvest的相關網站上找到的，我在文章最後會附上引用到的resource。\r\nsubmit_form2 <- function(session, form){\r\n  library(XML)\r\n  url <- XML::getRelativeURL(form$url, session$url)\r\n  url <- paste(url,'?',sep='')\r\n  values <- as.vector(rvest:::submit_request(form)$values)\r\n  att <- names(values)\r\n  if (tail(att, n=1) == \"NULL\"){\r\n    values <- values[1:length(values)-1]\r\n    att <- att[1:length(att)-1]\r\n  }\r\n  q <- paste(att,values,sep='=')\r\n  q <- paste(q, collapse = '&')\r\n  q <- gsub(\" \", \"+\", q)\r\n  url <- paste(url, q, sep = '')\r\n  html_session(url)\r\n}\r\n```\r\n\r\n```\r\n## 將session與form填入，並提交\r\nsubmit_form2(se,fo)\r\n```\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-05%20%E4%B8%8A%E5%8D%8811.13.04.png)\r\n\r\n此網址就是關鍵字輸入耳機搜尋到的網頁，接下來就是與前範例一樣進入這網頁解析並爬取資料。\r\n\r\n\r\n```\r\n## 讀入網頁html存入buy_page物件\r\nbuy_page <- read_html(\"http://search.ruten.com.tw/search/s000.php?enc=u&searchfrom=indexbar&k=%E8%80%B3%E6%A9%9F&_plid=57f4624e5dadc\")\r\n\r\n\r\n##\r\ndata_buy <- buy_page %>%\r\n  html_nodes(\".item-name-text\") %>% #這些css皆可運用SelectWidget來獲得\r\n  html_text() %>% \r\n  data.frame(產品標題 = .)\r\n  \r\n##\r\ndata_price <- buy_page %>%\r\n  html_nodes(\".rt-text-price\") %>%\r\n  html_text() %>% \r\n  data.frame(直購價 = .)\r\n  \r\n##\r\ndata_qotation <- buy_page %>%\r\n  html_nodes(\"tr:nth-child(2) td:nth-child(1) span\") %>%\r\n  html_text()  \r\n\r\n##\r\ndata_qotation <- data_qotation %>% \r\n  gsub(\" {0,}\",\"\",.) %>% \r\n  gsub(\"\\n\",\"\",.) %>% \r\n  gsub(\"最低運費：\",\"\",.) %>% \r\n  data.frame(最低運費 = .)\r\n  \r\n##\r\ndata_saler <- buy_page %>% \r\n  html_nodes(css = \".vtop a:nth-child(1)\") %>% \r\n  html_text() %>% \r\n  data.frame(賣家_ID = .)\r\n  \r\n##\r\ndata_buy %>% \r\n  bind_cols(data_price) %>% \r\n  bind_cols(data_qotation) %>% \r\n  bind_cols(data_saler) %>% View\r\n```\r\n\r\n![](https://raw.githubusercontent.com/Lofu/R-Crawl-Notebook/master/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-10%20%E4%B8%8B%E5%8D%883.35.56.png)\r\n\r\n\r\n## 小編心得雜談(可略)\r\n\r\n\r\n小弟最近的任務是爬取某一個網路商店中所有產品的產品簡介與內容，目的是為了拿來做時下流行的**文字探勘**，但其實小弟本身背景是一個統計分析的研究者，在爬取資料這一塊沒有任何經驗，所以在誤打誤撞的情況下尋找R相關的爬蟲套件，由於經常使用Hadley大大的套件進行資料清理、萃取與視覺化，所以就嘗試以他的作品`rvest`來試試，經過一連串爬文才有一些熟悉，然而，我真正的目的是寫出一支程式能夠把整個網路商店所有產品的簡介與內容自動爬下存成**json**檔，過程中必須經過：\r\n\r\n1. 前往目標網站熟悉整個網站的每一個部分，包括：每一種類別底下的類別其網址的變化\r\n2. 目標網頁中是否有分頁(必須做到爬蟲時能夠自動換頁)\r\n3. 爬取資料儲存格式的設定\r\n\r\n以上簡要列出幾點，但若各位大大看過以上的文，可以發現其實在基本的rvest運用上僅有運用**SelectWidget**來得到網頁目標的css，再將爬取下來的資料清理過後存成dataframe，再網上的文章中也鮮少提到其運用在自動化的爬蟲，故之後我便更換了工具來執行這項任務。但我認為，少量迅速的目標網頁資料，是能夠運用本文的方式來迅速地爬取並分析的！\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}